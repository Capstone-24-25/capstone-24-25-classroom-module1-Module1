---
title: "Biomarkers of ASD"
subtitle: ""
author: "Rebecca Chang, "
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

```{r}
# load any other packages and read data here
load('../data/biomarker-clean.RData')
library(tidyverse)
library(DiagrammeR)
library(ggplot2)
library(infer)
library(rsample)
library(randomForest)
library(tidymodels)
library(yardstick)
library(modelr)
```

## Abstract

Write a brief one-paragraph abstract that describes the contents of your write-up.

## Dataset

The data for this study were obtained from a cohort of 154 male pediatric subjects, including 76 with Autism Spectrum Disorder (ASD) and 78 typically developing (TD) boys. Serum samples were collected and analyzed using the SomaLogic SOMAScan platform, which measures the levels of 1,317 proteins. After quality control, 1,125 proteins were analyzed. The primary variables measured in this study include demographic information such as age, ethnicity, and co-morbid conditions. Other variables include the Autism Diagnostic Observation Schedule (ADOS) scores for ASD severity based on clinical assessment. Data preprocessing involved normalization and outlier handling. The protein abundance data were log10 transformed and z-transformed. Outliers were clipped to a specific range to mitigate their impact on analysis.

## Summary of published analysis

The study employed a multi-step approach to identify potential biomarkers for autism spectrum disorder (ASD). After the data was collected and preprocessed, a combination of three feature selection methods was used to identify a subset of proteins with the highest predictive power for ASD. These methods included random forest, t-tests, and correlation analysis with ADOS scores. By combining the top-ranked proteins from each method, a core set of 5 proteins was identified.

Finally, a logistic regression model was trained on the selected proteins to predict ASD status. The model's performance was evaluated using the area under the curve (AUC) metric. The optimal panel of 9 proteins, including the 5 core proteins and 4 additional proteins, achieved an AUC of 0.86, indicating high accuracy in distinguishing between ASD and TD cases.

```{r}
mermaid("
    graph LR
        A[Data Collection] --> B{Data Preprocessing}
        B --> C{Feature Selection}
        C --> D{Model Training}
        D --> E{Model Evaluation}
")
```

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers

#### Question 1

1.  The reason for log-transforming the protein levels in `biomarker-raw.csv` is likely to reduce skewness as protein levels often follow a skewed or non-normal distribution as seen in the histograms for a sample of proteins such as PACAP-27, TS, and LYNB. Protein levels can also have varying scales of measurement, which can lead to heteroscedasticity or non-constant variance. This can lead to issues with statistical analyses that assume normality. Log-transformation can help stabilize the variance which can in turn improve overall model performance.

    ```{r}
    var_names <- read_csv('../data/biomarker-raw.csv', 
                         col_names = F, 
                         n_max = 2, 
                         show_col_types = FALSE,
                         col_select = -(1:2)) %>%
      t() %>%
      as_tibble() %>%
      rename(name = V1, 
             abbreviation = V2) %>%
      na.omit()

    biomarker_not_transformed <- read_csv('../data/biomarker-raw.csv', 
             skip = 2,
             col_select = -2L,
             show_col_types = FALSE,
             col_names = c('group', 
                           'empty',
                           pull(var_names, abbreviation),
                           'ados'),
             na = c('-', '')) %>%
      filter(!is.na(group)) %>%
      # reorder columns
      select(group, ados, everything())


    # Select a sample of proteins
    set.seed(123)
    sample_proteins <- sample(colnames(biomarker_not_transformed)[3:ncol(biomarker_not_transformed)], 10)

    # Handle missing values 
    biomarker_not_transformed <- na.omit(biomarker_not_transformed)  

    # Melt the data for easier plotting
    biomarker_long <- biomarker_not_transformed %>%
      pivot_longer(cols = all_of(sample_proteins),
                   names_to = "protein",
                   values_to = "value")

    # Create a histogram facet plot
    ggplot(biomarker_long, aes(x = value)) +
      geom_histogram(fill = "steelblue", color = "black") +
      facet_wrap(~ protein, scales = "free_x") +
      labs(title = "Distribution of Selected Proteins",
           x = "Protein Value", y = "Frequency")
    ```

#### Question 2

### Methodlogical variations

#### Question 3

Original

```{r}
## MULTIPLE TESTING
####################

# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- biomarker_clean %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins
proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)

## RANDOM FOREST
##################

# store predictors and response separately
predictors <- biomarker_clean %>%
  select(-c(group, ados))

response <- biomarker_clean %>% pull(group) %>% factor()

# fit RF
set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

# check errors
rf_out$confusion

# compute importance scores
proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)

## LOGISTIC REGRESSION
#######################

# select subset of interest
proteins_sstar <- intersect(proteins_s1, proteins_s2)

biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

# partition into training and test set
set.seed(101422)
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

results <- testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
                truth = tr_c, pred,
                event_level = 'second')

results
```

Modification 1: Training Partition

```{r}
# Split the data into training and testing sets
set.seed(101422)
biomarker_split <- initial_split(biomarker_clean, prop = 0.8)
train_data <- training(biomarker_split)
test_data <- testing(biomarker_split)

## Multiple Testing
test_fn <- function(.df){
  t_test(.df,
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- train_data %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group,
               names_to = 'protein',
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>%
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins based on adjusted p-value
proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)

## Random Forest 
# store predictors and response separately
predictors <- train_data %>% 
  select(-c(group, ados))

response <- train_data %>% pull(group) %>% factor()

# fit RF
set.seed(123)
rf_out <- randomForest(x = predictors,
                       y = response,
                       ntree = 1000,
                       importance = T)

# check errors 
rf_out$confusion

# compute importance scores
proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)


## Logistic Regression
# select subset of interest
proteins_sstar <- intersect(proteins_s1, proteins_s2)

biomarker_sstar <- train_data %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

biomarker_sstar_test <- test_data %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = biomarker_sstar, 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

biomarker_sstar_test %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
              truth = tr_c, pred,
              event_level = 'second')
```

For this modification, proteins were selected by first training the data on the training group before testing the accuracy of the model using the testing group.

With this modification, each metric used to evaluate the accuracy of our classifier was lower. The ROC AUC decreased from 0.908 to 0.85. This indicates that this model has worse discriminatory power in distinguishing between the two classes. Additionally, before, there was a sensitivity of 0.875 and specificity of 0.8. However, this model achieved a sensitivity of 0.688 and a specificity of 0.733. This model had a much lower sensitivity, indicating that it is worse at identifying true positive cases (correctly identifying individuals with ASD) and also has a higher false positive rate.

Therefore, partitioning the data before analysis did not seem to improve results as the modification yield less data to be trained on.

Modification 2:

Modification 3:

### Improved classifier

#### Question 4
