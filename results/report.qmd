---
title: "Biomarkers of ASD"
subtitle: "If you want a subtitle put it here"
author: "List names here"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

Use this as a template. Keep the headers and remove all other text. In all, your report can be quite short. When it is complete, render and then push changes to your team repository.

```{r}
# load any other packages and read data here
library(tidyverse)
```

## Abstract

Write a brief one-paragraph abstract that describes the contents of your write-up.

## Dataset

Write a brief data description, including: how data were obtained; sample characteristics; variables measured; and data preprocessing. This can be largely based on the source paper and should not exceed 1-2 paragraphs.

## Summary of published analysis

Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for [GraphViz and Mermaid flowcharts](https://quarto.org/docs/authoring/diagrams.html).) Provide key results: the proteins selected for the classifier and the estimated accuracy.

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers

Tasks 1-2

After taking the log-transformed data, we counted the number of proteins that exceeded a transformed level of 3, which we would consider an outlier, for each subject in order identify subjects with outlier protein profiles. Below is a boxplot describing the distribution of the number of outlier proteins for each subjects divided between subjects with ASD and subjects considered typically developing(TD).

```{r Outlier-Graph, echo=FALSE, warning=FALSE, message=FALSE}
load("../data/biomarker_no_trim.RData")
biomarker_no_trim %>%
  ggplot(aes(x = group, y = outlier_proteins)) + geom_boxplot() + labs(x = 'Group', y = 'Number of Outlier Proteins in Each Subject', title = 'Distribution of Total of Outlier Proteins in Subjects by Group')

# summarize distributions
asd_profile <- biomarker_no_trim %>% 
  filter(group == 'ASD') %>% 
  pull(outlier_proteins)

td_profile <- biomarker_no_trim %>% 
  filter(group == 'TD') %>% 
  pull(outlier_proteins)
```

The group of subjects with ASD had a total of `r sum(asd_profile)` outlier proteins while the typically developing group had larger a total of `r sum(td_profile)`. The graph shows similar distributions with typically developing subjects having a mean total of **`r round(mean(td_profile), 2)`** outlier proteins, and subjects with ASD having a lower mean total of **`r round(mean(asd_profile), 2)`**. Both groups share a median total of proteins per subject of **`r median(asd_profile)`**.

We also examined the proteins most prone to being outliers:

```{r Outlier-Proteins, echo=FALSE, warning=FALSE, message=FALSE}
outlier_count <-(biomarker_no_trim[, -c(1, 2, 3)] >= 3) %>% 
  colSums() %>% 
  as.data.frame() %>% 
  rename(outliers = 1) %>% 
  arrange(desc(outliers)) 

outlier_count %>% 
  ggplot(aes(outliers)) + geom_histogram(binwidth = 1, fill='lightblue3') + labs(x = "Number of Outlier Subjects per Protein", y = 'Count', title = "Distribution of Number of Outlier Subjects per Protein")

outlier_groups <- outlier_count %>% 
  group_by(outliers) %>% 
  count()
```

**A total of 722 of the proteins have 0 or 1 outliers across subjects.** The proteins with the highest count of outliers across subjects include **TLR2 with 7 outlier values**, and **IL-13, TGM3, GM-CSF, and hnRNP K, and SOD3 each with 6 outlier values.**

### Methodological variations

The benchmark protein panel was using 4 proteins which resulted in a sensitivity of 0.875 and accuracy of 0.839. Our goal now is to determine alternate methods to extract a comparable panel that can reproduce similar resulting values. First we will partition the data into a training and testing set, using the training data to select the protein panel. Using a t-test and random forest variable importance we extract two different protein panels which we will then determine our final panel by selecting intersecting proteins from both tests. Our result is 4 selected proteins, which when fitted to the testing data returned a sensitivity of 0.769 and accuracy of 0.677. Despite using the same method to derive our protein panel, our panel provides a weaker accuracy of the data. This is most likely due to the variability of a training and testing set. Because we do not have the entire data to find significant proteins, it leaves room for error when finding proteins.

The next step is to attempt a panel with more proteins. For both the t-test and logistical regression, we selected 25 potentially significant proteins from both to then intersect and find our panel. This method produced us a panel of 13 proteins, with a resulting sensitivity of 0.8125 and accuracy of 0.839, significantly better than our smaller panel of 4 proteins. Our results have improved significantly as we have included additional variables to estimate the result. The additional proteins we employed in testing may still have some significance towards our estimates, hence why it performed better, just because they weren't significant enough they still have some correlation to the outcome.

Finally, we will make a panel using the original selection of 10 proteins using t-testing and random forests, but we will employ fuzzy intersection to determine the final panel. The result is a panel of 6 proteins that have a sensitivity of 0.75 and a accuracy of 0.774 after testing on the testing data. Using fuzzy intersection did not improve estimates to the same level of our benchmark, but it still produced pretty impressive accuracy results. Compared to our first panel of 4 proteins the fuzzy intersection had worse sensitivity, but had much more accuracy.

### Improved classifier

Now that we have looked at methodical variations with the way we select our protein panel, we will now attempt to find a panel that has better accuracy than the benchmark panel.

Our first attempt was to use correlation to determine significant proteins. As in the before section, we will partition the data first to create a training and testing set, and filter the data so we are just looking at participants who have ASD. We then determined the correlation of each variable, selecting the 2 proteins that had the highest absolute value of the correlation. Our resulting accuracy was 0.613 when testing on the testing data, so while this was a simpler panel, its estimates were far from comparable to our benchmark panel.

Our second attempt was using LASSO regression to find our panel. First we tuned and fitted the Lasso model using deviance as our measure. We then found a lambda, e\^(-1.8), that how a low deviance and allowed for a small protein panel. The result of using this lambda was a panel of 3 proteins, which when tested on the testing set returned an accuracy of 0.871. This result improved upon our benchmark of 0.831 with a smaller panel, so we were successful in finding a simple panel with comparable results.
