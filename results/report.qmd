---
title: "Biomarkers of ASD"
subtitle: ""
author: "Rebecca Chang, "
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

```{r}
# load any other packages and read data here
load('../data/biomarker-clean.RData')
library(tidyverse)
library(DiagrammeR)
library(ggplot2)
library(infer)
library(rsample)
library(randomForest)
library(tidymodels)
library(yardstick)
library(modelr)
```

## Abstract

## Dataset

The data for this study were obtained from a cohort of 154 male pediatric subjects, including 76 with Autism Spectrum Disorder (ASD) and 78 typically developing (TD) boys. Serum samples were collected and analyzed using the SomaLogic SOMAScan platform, which measures the levels of 1,317 proteins. After quality control, 1,125 proteins were analyzed. The primary variables measured in this study include demographic information such as age, ethnicity, and co-morbid conditions. Other variables include the Autism Diagnostic Observation Schedule (ADOS) scores for ASD severity based on clinical assessment. Data preprocessing involved normalization and outlier handling. The protein abundance data were log10 transformed and z-transformed. Outliers were clipped to a specific range to mitigate their impact on analysis.

## Summary of published analysis

The study employed a multi-step approach to identify potential biomarkers for autism spectrum disorder (ASD). After the data was collected and preprocessed, a combination of three feature selection methods was used to identify a subset of proteins with the highest predictive power for ASD. These methods included random forest, t-tests, and correlation analysis with ADOS scores. By combining the top-ranked proteins from each method, a core set of 5 proteins was identified.

Finally, a logistic regression model was trained on the selected proteins to predict ASD status. The model's performance was evaluated using the area under the curve (AUC) metric. The optimal panel of 9 proteins, including the 5 core proteins and 4 additional proteins, achieved an AUC of 0.86, indicating high accuracy in distinguishing between ASD and TD cases.

```{r}
mermaid("
    graph LR
        A[Data Collection] --> B{Data Preprocessing}
        B --> C{Feature Selection}
        C --> D{Model Training}
        D --> E{Model Evaluation}
")
```

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers

#### Question 1

1.  The reason for log-transforming the protein levels in `biomarker-raw.csv` is likely to reduce skewness as protein levels often follow a skewed or non-normal distribution as seen in the histograms for a sample of proteins such as PACAP-27, TS, and LYNB. Protein levels can also have varying scales of measurement, which can lead to heteroscedasticity or non-constant variance. This can lead to issues with statistical analyses that assume normality. Log-transformation can help stabilize the variance which can in turn improve overall model performance.

    ```{r, error=FALSE}
    var_names <- read_csv('../data/biomarker-raw.csv', 
                         col_names = F, 
                         n_max = 2, 
                         col_select = -(1:2)) %>%
      t() %>%
      as_tibble() %>%
      rename(name = V1, 
             abbreviation = V2) %>%
      na.omit()

    biomarker_not_transformed <- read_csv('../data/biomarker-raw.csv', 
             skip = 2,
             col_select = -2L,
             col_names = c('group', 
                           'empty',
                           pull(var_names, abbreviation),
                           'ados'),
             na = c('-', '')) %>%
      filter(!is.na(group)) %>%
      # reorder columns
      select(group, ados, everything())


    # Select a sample of proteins
    set.seed(123)
    sample_proteins <- sample(colnames(biomarker_not_transformed)[3:ncol(biomarker_not_transformed)], 10)

    # Check data types (optional)
    str(biomarker_not_transformed[, sample_proteins])  # View data types of selected proteins

    # Handle missing values (optional)
    biomarker_not_transformed <- na.omit(biomarker_not_transformed)  # Alternatively, impute missing values

    # Melt the data for easier plotting
    biomarker_long <- biomarker_not_transformed %>%
      pivot_longer(cols = all_of(sample_proteins),
                   names_to = "protein",
                   values_to = "value")

    # Create a histogram facet plot
    ggplot(biomarker_long, aes(x = value)) +
      geom_histogram(fill = "steelblue", color = "black") +
      facet_wrap(~ protein, scales = "free_x") +
      labs(title = "Distribution of Selected Proteins",
           x = "Protein Value", y = "Frequency")
    ```

#### Question 2

### Methodlogical variations

#### Question 3

Modification 1: Training Partition

```{r}
# Split the data into training and testing sets
set.seed(123)
biomarker_split <- initial_split(biomarker_clean, prop = 0.8)
train_data <- training(biomarker_split)
test_data <- testing(biomarker_split)

## Multiple Testing
test_fn <- function(.df){
  t_test(.df,
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- train_data %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group,
               names_to = 'protein',
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>%
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins based on adjusted p-value
proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)

## Random Forest 
# store predictors and response separately
predictors <- train_data %>% 
  select(-c(group, ados))

response <- train_data %>% pull(group) %>% factor()

# fit RF
set.seed(123)
rf_out <- randomForest(x = predictors,
                       y = response,
                       ntree = 1000,
                       importance = T)

# check errors 
rf_out$confusion

# compute importance scores
proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)


## Logistic Regression
# select subset of interest
proteins_sstar <- intersect(proteins_s1, proteins_s2)

biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

# partition into training and test set
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
              truth = tr_c, pred,
              event_level = 'second')
```

With this modification, the results were different from the article in that the AUC increased to 0.906, compared to the reported AUC of 0.86 before. This indicates that this model has better discriminatory power in distinguishing between the two classes. Additionally, the article reports a sensitivity of 0.83 and a specificity of 0.84. However, this model achieved a sensitivity of 0.846 and a specificity of 0.778. While both models perform well, this model has slightly higher sensitivity, indicating it's better at identifying true positive cases (correctly identifying individuals with ASD), but slightly lower specificity, meaning it might have a higher false positive rate.

Modification 2:

Modification 3:

### Improved classifier

#### Question 4
